# -*- coding: utf-8 -*-
"""NewLLM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hzyqVg45AleVELKHSzmyTCamNKZv_eD2
"""

!pip install transformers==4.46.2 peft==0.13.2 accelerate==1.1.1 trl==0.12.1 bitsandbytes==0.45.2 datasets==3.1.0 huggingface-hub==0.26.2 safetensors==0.4.5 -q

import torch
from datasets import load_dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments,
    DataCollatorForLanguageModeling,
    pipeline,
    logging,
)
from peft import LoraConfig
from trl import SFTTrainer

# Base model (from Hugging Face Hub)
model_name = "NousResearch/Llama-2-7b-chat-hf"

# Instruction dataset
dataset_name = "TejasKoti/ProjectDataset"

# Fine-tuned model output directory
new_model = "Llama-2-7b-chat-finetune"

################################################################################
# QLoRA parameters
################################################################################
lora_r = 64
lora_alpha = 16
lora_dropout = 0.1

################################################################################
# bitsandbytes config
################################################################################
use_4bit = True
bnb_4bit_compute_dtype = "float16"
bnb_4bit_quant_type = "nf4"
use_nested_quant = False

################################################################################
# TrainingArguments parameters
################################################################################
output_dir = "./results"
num_train_epochs = 5
fp16 = True
bf16 = False
per_device_train_batch_size = 4
per_device_eval_batch_size = 4
gradient_accumulation_steps = 1
gradient_checkpointing = True
max_grad_norm = 0.3
learning_rate = 2e-4
weight_decay = 0.001
optim = "paged_adamw_32bit"
lr_scheduler_type = "cosine"
max_steps = -1
warmup_ratio = 0.03
group_by_length = True
save_steps = 0
logging_steps = 25

################################################################################
# Dataset prep
################################################################################
dataset = load_dataset(dataset_name, split="train")
print("Dataset columns:", dataset.column_names)

# ⚠️ Replace this with the actual column name from your dataset
text_column = "train"

compute_dtype = getattr(torch, bnb_4bit_compute_dtype)
bnb_config = BitsAndBytesConfig(
    load_in_4bit=use_4bit,
    bnb_4bit_quant_type=bnb_4bit_quant_type,
    bnb_4bit_compute_dtype=compute_dtype,
    bnb_4bit_use_double_quant=use_nested_quant,
)

# Load base model
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto"
)
model.config.use_cache = False

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

# LoRA config
peft_config = LoraConfig(
    lora_alpha=lora_alpha,
    lora_dropout=lora_dropout,
    r=lora_r,
    bias="none",
    task_type="CAUSAL_LM",
)

# Training args
training_arguments = TrainingArguments(
    output_dir=output_dir,
    num_train_epochs=num_train_epochs,
    per_device_train_batch_size=per_device_train_batch_size,
    gradient_accumulation_steps=gradient_accumulation_steps,
    optim=optim,
    save_steps=save_steps,
    logging_steps=logging_steps,
    learning_rate=learning_rate,
    weight_decay=weight_decay,
    fp16=fp16,
    bf16=bf16,
    max_grad_norm=max_grad_norm,
    max_steps=max_steps,
    warmup_ratio=warmup_ratio,
    group_by_length=group_by_length,
    lr_scheduler_type=lr_scheduler_type,
    report_to="tensorboard"
)

# Collator
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False
)

# Tokenization
def preprocess_function(examples):
    return tokenizer(examples[text_column], padding="max_length", truncation=True)

processed_dataset = dataset.map(preprocess_function, batched=True)

# Trainer
trainer = SFTTrainer(
    model=model,
    train_dataset=processed_dataset,
    args=training_arguments,
    peft_config=peft_config,
    data_collator=data_collator,
)

# Train
trainer.train()

# Save model + tokenizer
trainer.model.save_pretrained(new_model)
tokenizer.save_pretrained(new_model)

print(f"✅ Fine-tuned model saved to {new_model}")

# Silence warnings
logging.set_verbosity(logging.CRITICAL)

# Quick test
prompt = "Explain labour laws"
pipe = pipeline("text-generation", model=trainer.model, tokenizer=tokenizer, max_length=500)
result = pipe(f"<s>[INST] {prompt} [/INST]")
print("Generated output:", result[0]['generated_text'])

import locale
locale.getpreferredencoding = lambda: "UTF-8"

from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

base_model_name = "NousResearch/Llama-2-7b-chat-hf"
adapter_path = "./Llama-2-7b-chat-finetune"
save_path = "./lawyer-llama-merged"

# Load base model
base_model = AutoModelForCausalLM.from_pretrained(
    base_model_name,
    torch_dtype="auto",
    device_map="auto"
)

# Attach adapter (let PEFT handle the mapping)
model = PeftModel.from_pretrained(base_model, adapter_path, is_trainable=False)

# Merge LoRA
model = model.merge_and_unload()

# Save tokenizer + merged model
tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.save_pretrained(save_path)
model.save_pretrained(save_path)

print("✅ Merged and saved at", save_path)

from huggingface_hub import login

# 1. Login (you'll paste your HF token here)
!huggingface-cli login

# 2. Push model and tokenizer
save_path = "./lawyer-llama-merged"   # the folder you saved merged model + tokenizer

model.push_to_hub("TejasKoti/Llama-2-7b-lawyer-chat", commit_message="Merged LoRA with base model")
tokenizer.push_to_hub("TejasKoti/Llama-2-7b-lawyer-chat", commit_message="Tokenizer for merged model")