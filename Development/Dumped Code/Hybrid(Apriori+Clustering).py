# -*- coding: utf-8 -*-
"""Hybrid-Apriori&Clustering.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SNJqYlyAR78FmfktY863RoEZuc23ayEG
"""

from bs4 import BeautifulSoup
import requests
import csv

# Base URL of the website to scrape (excluding page number)
base_url = 'https://kanoongurus.com/search?lawyerName=&city=&state=&page='

# Define the number of pages to scrape (you can change this based on the total pages available)
total_pages = 72  # Modify this according to the total number of pages on the site

# Open a CSV file to save the dataset
with open('lawyers_dataset.csv', mode='w', newline='') as file:
    writer = csv.writer(file)
    writer.writerow(['Name', 'Experience', 'Specialization', 'Location', 'Rating', 'Profile URL'])

    # Loop through each page
    for page in range(1, total_pages + 1):
        # Construct the full URL for the current page
        url = base_url + str(page)

        # Send an HTTP request to the URL
        response = requests.get(url)
        html_content = response.content

        # Parse the HTML content using BeautifulSoup
        soup = BeautifulSoup(html_content, 'html.parser')

        # Find all lawyer blocks
        lawyer_blocks = soup.find_all('div', class_='bs-box')

        # Loop through each lawyer block and extract the required details
        for block in lawyer_blocks:
            # Extract lawyer's name
            name = block.find('div', class_='custom-value-name').text.strip()

            # Extract experience
            experience = block.find('div', class_='custom-label', text='Experience').find_next_sibling('div').text.strip()

            # Extract specialization
            specialization = block.find('div', class_='custom-label', text='Specialization').find_next_sibling('div').text.strip()

            # Extract location
            location = block.find('div', class_='custom-label', text='Location').find_next_sibling('div').text.strip()

            # Extract rating
            rating_block = block.find('div', class_='rating-tag')
            rating = rating_block.text.strip() if rating_block else 'No rating'

            # Extract profile URL
            profile_url = block.find('a')['href']
            full_profile_url = f"https://kanoongurus.com{profile_url}"

            # Write the extracted data to the CSV file
            writer.writerow([name, experience, specialization, location, rating, full_profile_url])

        print(f"Data from page {page} extracted successfully.")

print("Data extraction completed for all pages. Check the 'lawyers_dataset.csv' file.")

!pip install mlxtend

!pip install --upgrade mlxtend

!pip install tabulate

import pandas as pd
from mlxtend.frequent_patterns import apriori
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from tabulate import tabulate
import numpy as np

# Load the dataset
file_path = 'lawyers_dataset.csv'
lawyers_df = pd.read_csv(file_path)

# Preprocess the dataset
lawyers_df['Experience'] = lawyers_df['Experience'].astype(str).fillna('0 Years')
lawyers_df['Experience'] = lawyers_df['Experience'].str.extract(r'(\d+)').astype(int)
lawyers_df['Specialization'] = lawyers_df['Specialization'].fillna('')
lawyers_df['Specialization List'] = lawyers_df['Specialization'].str.split(',')

# Generate a binary matrix for Apriori algorithm
specialization_list = [spec.strip() for sublist in lawyers_df['Specialization List'] for spec in sublist]
specialization_set = list(set(specialization_list))
binary_matrix = pd.DataFrame(0, index=lawyers_df['Name'], columns=specialization_set)

for i, row in lawyers_df.iterrows():
    for spec in row['Specialization List']:
        binary_matrix.loc[row['Name'], spec.strip()] = 1

# Apply Apriori algorithm
frequent_itemsets = apriori(binary_matrix, min_support=0.1, use_colnames=True)

# Input from user
user_location = input("Enter your location: ")
user_specializations = input("Enter the areas of legal practice (comma-separated): ").split(',')

# Filter dataset based on location and user input
filtered_df = lawyers_df[lawyers_df['Location'].str.contains(user_location, case=False, na=False)]
filtered_df = filtered_df[filtered_df['Specialization'].str.contains('|'.join(user_specializations), case=False, na=False)]

# Combine experience and specialization count for ranking
filtered_df['Specialization Count'] = filtered_df['Specialization List'].apply(len)
filtered_df['Score'] = filtered_df['Experience'] + filtered_df['Specialization Count']

# Scale and cluster lawyers to find the best fit using machine learning
scaler = StandardScaler()
features = scaler.fit_transform(filtered_df[['Experience', 'Specialization Count']])

# Dynamically determine the number of clusters
n_clusters = min(3, len(filtered_df))
if n_clusters > 1:
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    filtered_df['Cluster'] = kmeans.fit_predict(features)
else:
    filtered_df['Cluster'] = 0  # Assign all to a single cluster if only one lawyer

# Sort lawyers by score within each cluster
recommended_lawyers = filtered_df.sort_values(['Cluster', 'Score'], ascending=[True, False])

# Display the top 3 lawyers using Tabulate
top_3_lawyers = recommended_lawyers.head(3)
if len(top_3_lawyers) > 0:
    print("Top Recommended Lawyers:")
    print(tabulate(top_3_lawyers[['Name', 'Experience', 'Specialization', 'Location', 'Score']],
                   headers='keys',
                   tablefmt='grid',
                   showindex=False))
else:
    print("No lawyers found matching the given criteria.")

!pip install sympy
!pip install sentence-transformers
!pip install mlxtend

import pandas as pd
from sentence_transformers import SentenceTransformer, util
from tabulate import tabulate
import re
from mlxtend.frequent_patterns import apriori
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans

# Load the dataset
file_path = "lawyers_dataset.csv"  # Make sure it's uploaded to Colab
df = pd.read_csv(file_path)

df.columns = [col.lower().strip() for col in df.columns]

def extract_experience(exp_str):
    match = re.search(r"(\d+)", str(exp_str))
    return int(match.group(1)) if match else 0

df["experience"] = df["experience"].apply(extract_experience)

df["specialization"] = df["specialization"].fillna("")
df["specialization_list"] = df["specialization"].str.split(',')

# Generate binary matrix for Apriori
specialization_list = [spec.strip() for sublist in df['specialization_list'] for spec in sublist]
specialization_set = list(set(specialization_list))
binary_matrix = pd.DataFrame(0, index=df['name'], columns=specialization_set)
for i, row in df.iterrows():
    for spec in row['specialization_list']:
        binary_matrix.loc[row['name'], spec.strip()] = 1

frequent_itemsets = apriori(binary_matrix, min_support=0.1, use_colnames=True)

# Load embedding model
model = SentenceTransformer("all-MiniLM-L6-v2")

def parse_query(query):
    # Normalize aliases
    spec_aliases = {
        "crime section": "criminal law",
        "crime": "criminal law",
        "divorce": "family law",
        "property": "real estate law",
        "cyber": "cyber law"
    }
    for alias, canonical in spec_aliases.items():
        query = query.replace(alias, canonical)

    min_experience = None
    max_experience = None
    excluded_locations = []
    required_specialization = None
    excluded_specializations = []
    user_location = None
    user_specializations = []

    more_than_match = re.search(r"(?:more than|at least|min(?:imum)?) (\d+) years?", query, re.IGNORECASE)
    less_than_match = re.search(r"(?:less than|max(?:imum)?) (\d+) years?", query, re.IGNORECASE)

    if more_than_match:
        min_experience = int(more_than_match.group(1)) + 1
    if less_than_match:
        max_experience = int(less_than_match.group(1)) - 1

    excluded_locations = re.findall(r"(?:not in|except|avoid) (\w+)", query, re.IGNORECASE)

    spec_match = re.search(r"(?:practicing|having|specializing in|handling|working on|doing) ([\w\s]+ law)", query, re.IGNORECASE)
    if spec_match:
        required_specialization = spec_match.group(1).strip()

    excluded_specializations = re.findall(r"(?:not|avoid|except|without) ([\w\s]+ law)", query, re.IGNORECASE)

    loc_match = re.search(r"(?:in|located in|based in|from) (\w+)", query, re.IGNORECASE)
    if loc_match:
        user_location = loc_match.group(1)

    user_specializations = re.findall(r"(?:focus on|expert in|handles|deals with) ([\w\s]+ law)", query, re.IGNORECASE)

    return min_experience, max_experience, excluded_locations, required_specialization, excluded_specializations, user_location, user_specializations

def query_database(user_query):
    min_experience, max_experience, excluded_locations, required_specialization, excluded_specializations, user_location, user_specializations = parse_query(user_query)

    filtered_df = df.copy()

    if min_experience is not None:
        filtered_df = filtered_df[filtered_df["experience"] >= min_experience]
    if max_experience is not None:
        filtered_df = filtered_df[filtered_df["experience"] <= max_experience]

    if user_location:
        filtered_df = filtered_df[filtered_df["location"].str.contains(user_location, case=False, na=False)]
    if user_specializations:
        filtered_df = filtered_df[filtered_df["specialization"].str.contains('|'.join(user_specializations), case=False, na=False)]

    if filtered_df.empty:
        print("No lawyers found matching the given criteria.")
        return

    filtered_df["location_penalty"] = filtered_df["location"].apply(lambda loc: -0.2 if any(exc.lower() in loc.lower() for exc in excluded_locations) else 0)
    filtered_df["spec_bonus"] = filtered_df["specialization"].apply(lambda spec: 0.2 if required_specialization and required_specialization.lower() in spec.lower() else 0)
    filtered_df["spec_penalty"] = filtered_df["specialization"].apply(lambda spec: -0.2 if any(exc.lower() in spec.lower() for exc in excluded_specializations) else 0)

    query_embedding = model.encode(user_query, convert_to_tensor=True).cpu()

    def get_embedding(row):
        text = f"{row['name']} {row['experience']} years experience in {row['specialization']} at {row['location']}"
        return model.encode(text, convert_to_tensor=True).cpu()

    filtered_df["embedding"] = filtered_df.apply(get_embedding, axis=1)
    filtered_df["similarity"] = filtered_df["embedding"].apply(lambda emb: util.pytorch_cos_sim(query_embedding, emb).item())
    filtered_df["final_score"] = filtered_df["similarity"] + filtered_df["spec_bonus"] + filtered_df["location_penalty"] + filtered_df["spec_penalty"]

    filtered_df["specialization_count"] = filtered_df["specialization_list"].apply(len)
    filtered_df["score"] = filtered_df["experience"] + filtered_df["specialization_count"]

    scaler = StandardScaler()
    features = scaler.fit_transform(filtered_df[["experience", "specialization_count"]])

    n_clusters = min(3, len(filtered_df))
    if n_clusters > 1:
        kmeans = KMeans(n_clusters=n_clusters, random_state=42)
        filtered_df["cluster"] = kmeans.fit_predict(features)
    else:
        filtered_df["cluster"] = 0

    filtered_df["final_rank"] = filtered_df["final_score"] + (filtered_df["score"] / 10)
    recommended_lawyers = filtered_df.sort_values(["cluster", "final_rank"], ascending=[True, False])

    top_3_lawyers = recommended_lawyers.head(3)
    if len(top_3_lawyers) > 0:
        print("Top Recommended Lawyers:")
        print(tabulate(top_3_lawyers[["name", "experience", "specialization", "location", "final_rank"]], headers='keys', tablefmt='grid', showindex=False))
    else:
        print("No lawyers found matching the given criteria.")

# Example usage
user_query = input("Enter your query: ")
query_database(user_query)

import pandas as pd
import re
from sentence_transformers import SentenceTransformer, util
from tabulate import tabulate
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans

# Load dataset
file_path = "lawyers_dataset.csv"
df = pd.read_csv(file_path)
df.columns = [col.lower().strip() for col in df.columns]

# Clean & prepare data
df["experience"] = df["experience"].fillna("0").astype(str).str.extract(r"(\d+)").fillna(0).astype(int)
df["specialization"] = df["specialization"].fillna("")
df["specialization_list"] = df["specialization"].str.lower().str.split(r',\s*', regex=True)
df["city"] = df["location"].fillna("").astype(str).apply(lambda x: x.split(",")[0].strip().lower())

# Embedding model
model = SentenceTransformer("all-MiniLM-L6-v2")

# Parser function to extract query components
def parse_query(query):
    query = query.lower()
    min_exp, max_exp, exact_exp = None, None, None
    include_cities, exclude_cities = [], []
    include_specs, exclude_specs = [], []

    # Experience
    if match := re.search(r"between (\d+)\s*(?:years?)?\s*(?:and|to)\s*(\d+)\s*(?:years?)?", query):
        min_exp, max_exp = int(match[1]), int(match[2])
    if match := re.search(r"only (\d+) years?", query):
        exact_exp = int(match[1])
    if match := re.search(r"more than (\d+)", query):
        min_exp = int(match[1]) + 1
    if match := re.search(r"less than (\d+)", query):
        max_exp = int(match[1]) - 1

    # Location inclusions and exclusions
    city_pattern = r"mumbai|chennai|delhi|kolkata|bangalore|jaipur|hyderabad|patna|noida|ghaziabad|pune|ranchi|allahabad|indore|nagpur|bhubaneswar|dehradun|gurgaon|shimla|jabalpur|chandigarh|nainital|jodhpur|mathura|meerut|calcutta|panaji|howrah|vapi|guwahati|varanasi|bilaspur|sonipat|eluru|shahdol|rudrapur|angul|kotdwara|shivpuri|malda|dhule|cuttack|hoshiarpur|mohali|central delhi|south delhi|new delhi|faridabad|agartala|gangtok|bahraich|alwar"
    city_block = re.findall(rf"(in|from|not in|not) ((?:{city_pattern})(?:\s*(?:and|,)\s*{city_pattern})*)", query)
    for prefix, group in city_block:
        cities = re.split(r"\s*(?:and|,)\s*", group)
        if "not" in prefix:
            exclude_cities += cities
        else:
            include_cities += cities

    # Specialization inclusions and exclusions
    if match := re.search(r"(specializing in|only|do|handles|focus(?: on)?) ([a-z\s]+(?: or [a-z\s]+)*)", query):
        spec_str = match[2]
        spec_str = re.sub(r"not in .*", "", spec_str)  # Remove trailing city info
        specs = re.split(r"\s*or\s*", spec_str)
        include_specs += [s.strip() for s in specs]

    exclude_specs += re.findall(r"not in ([a-z\s]+) specialization", query)
    exclude_specs += re.findall(r"without ([a-z\s]+)", query)
    exclude_specs += re.findall(r"not ([a-z\s]+ law)", query)

    return {
        "min_exp": min_exp, "max_exp": max_exp, "exact_exp": exact_exp,
        "include_cities": [c.strip() for c in include_cities if c.strip()],
        "exclude_cities": [c.strip() for c in exclude_cities if c.strip()],
        "include_specs": [s.strip() for s in include_specs if s.strip()],
        "exclude_specs": [s.strip() for s in exclude_specs if s.strip()]
    }

# Main query engine
def query_lawyers(user_query):
    filters = parse_query(user_query)
    print("[DEBUG] Parsed Query:", filters)

    fdf = df.copy()

    if filters["exact_exp"] is not None:
        fdf = fdf[fdf["experience"] == filters["exact_exp"]]
    else:
        if filters["min_exp"] is not None:
            fdf = fdf[fdf["experience"] >= filters["min_exp"]]
        if filters["max_exp"] is not None:
            fdf = fdf[fdf["experience"] <= filters["max_exp"]]

    if filters["include_cities"]:
        fdf = fdf[fdf["city"].isin([c.lower() for c in filters["include_cities"]])]
    if filters["exclude_cities"]:
        fdf = fdf[~fdf["city"].isin([c.lower() for c in filters["exclude_cities"]])]

    if filters["include_specs"]:
        fdf = fdf[fdf["specialization_list"].apply(
            lambda specs: any(any(inc in s or s in inc for s in specs) for inc in filters["include_specs"])
        )]

    if filters["exclude_specs"]:
        fdf = fdf[~fdf["specialization_list"].apply(
            lambda specs: any(any(exc in s or s in exc for s in specs) for exc in filters["exclude_specs"])
        )]

    if fdf.empty:
        print("No lawyers found matching the criteria.")
        return

    # Semantic similarity scoring
    query_embedding = model.encode(user_query, convert_to_tensor=True).cpu()
    def embed(row):
        text = f"{row['name']} {row['experience']} years {row['specialization']} {row['location']}"
        return model.encode(text, convert_to_tensor=True).cpu()

    fdf["embedding"] = fdf.apply(embed, axis=1)
    fdf["similarity"] = fdf["embedding"].apply(lambda emb: util.pytorch_cos_sim(query_embedding, emb).item())
    fdf["rank"] = fdf["similarity"] + (fdf["experience"] / 50)

    result = fdf.sort_values("rank", ascending=False).head(5)
    print("\nTop Recommended Lawyers:")
    print(tabulate(result[["name", "experience", "specialization", "location", "rank"]], headers="keys", tablefmt="grid", showindex=False))

# Run the engine
user_query = input("Enter your query: ")
query_lawyers(user_query)

import pandas as pd
from mlxtend.frequent_patterns import apriori
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt
import numpy as np

# Load the dataset
df = pd.read_csv('lawyers_dataset.csv')
df['Experience'] = df['Experience'].astype(str).fillna('0 Years').str.extract(r'(\d+)').astype(float).fillna(0)
df['Specialization'] = df['Specialization'].fillna('')
df['Specialization List'] = df['Specialization'].str.split(',')

# Binary matrix for Apriori
specialization_list = [spec.strip() for sublist in df['Specialization List'] for spec in sublist]
specialization_set = list(set(specialization_list))
binary_matrix = pd.DataFrame(0, index=df['Name'], columns=specialization_set)
for i, row in df.iterrows():
    for spec in row['Specialization List']:
        binary_matrix.loc[row['Name'], spec.strip()] = 1

# 1. Apriori support vs. number of frequent itemsets (as a proxy for 'accuracy')
support_values = np.linspace(0.05, 0.5, 10)
frequent_counts = []
for s in support_values:
    itemsets = apriori(binary_matrix, min_support=s, use_colnames=True)
    frequent_counts.append(len(itemsets))

# 2. Clustering: KMeans accuracy using silhouette score
features = df[['Experience']]
df['Specialization Count'] = df['Specialization List'].apply(len)
features['Specialization Count'] = df['Specialization Count']
scaler = StandardScaler()
X_scaled = scaler.fit_transform(features)

k_values = range(2, 11)
silhouette_scores = []
for k in k_values:
    kmeans = KMeans(n_clusters=k, random_state=42)
    labels = kmeans.fit_predict(X_scaled)
    silhouette_scores.append(silhouette_score(X_scaled, labels))

# 3. Combined accuracy (standardized counts + scores)
combined_score = [f / max(frequent_counts) + s / max(silhouette_scores)
                  for f, s in zip(frequent_counts[:len(silhouette_scores)], silhouette_scores)]

# Plotting
plt.figure()
plt.plot(support_values, frequent_counts, marker='o')
plt.title("Apriori: Support vs. Number of Frequent Itemsets")
plt.xlabel("Min Support")
plt.ylabel("Number of Frequent Itemsets")
plt.grid(True)
plt.show()

plt.figure()
plt.plot(k_values, silhouette_scores, marker='o')
plt.title("KMeans: Number of Clusters vs. Silhouette Score")
plt.xlabel("Number of Clusters")
plt.ylabel("Silhouette Score")
plt.grid(True)
plt.show()

plt.figure()
plt.plot(k_values, combined_score, marker='o')
plt.title("Combined Accuracy: Normalized Apriori + KMeans Score")
plt.xlabel("Index (Matching k and support)")
plt.ylabel("Combined Score")
plt.grid(True)
plt.show()