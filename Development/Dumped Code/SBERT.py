# -*- coding: utf-8 -*-
"""BERT_Recomm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16NjjlH7bfP6TlnR402q3yLv0lW7W771b
"""

!pip install transformers torch

import re
import pandas as pd
import torch
from sklearn.metrics.pairwise import cosine_similarity
from transformers import BertTokenizer, BertModel

# Load BERT model and tokenizer
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
bert_model = BertModel.from_pretrained("bert-base-uncased")
bert_model.eval()
print("BERT model loaded.")

# Function to get BERT sentence embedding (mean of last hidden states)
def get_bert_embedding(text):
    with torch.no_grad():
        inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=512)
        outputs = bert_model(**inputs)
        last_hidden_state = outputs.last_hidden_state  # shape: [1, seq_len, hidden_size]
        embedding = last_hidden_state.mean(dim=1)      # shape: [1, hidden_size]
    return embedding.squeeze().numpy()

# Load data
df = pd.read_csv("lawyers_with_estimated_price.csv")

# Create summary column
def create_summary(row):
    return (
        f"{row['Name']} is a lawyer specializing in {row['Domain']} with "
        f"{row['Years of active experience']} years of experience. "
        f"Charges approximately ₹{row['Price']:.0f} and has a client satisfaction rating of "
        f"{row['Client satisfaction (out of 10)']}/10."
    )

df["summary_text"] = df.apply(create_summary, axis=1)

# Precompute BERT embeddings for all summaries
print("Generating embeddings for all lawyers...")
df["embedding"] = df["summary_text"].apply(get_bert_embedding)

# Recommendation function
def recommend(query):
    filtered_df = df.copy()
    query_lower = query.lower()

    # Experience filter
    exp_match = re.search(r"(less|more) than (\d+)\s*years?", query_lower)
    if exp_match:
        op, val = exp_match.groups()
        val = int(val)
        if op == "less":
            filtered_df = filtered_df[filtered_df["Years of active experience"] < val]
        else:
            filtered_df = filtered_df[filtered_df["Years of active experience"] > val]

    # Budget filter
    budget_match = re.search(r"(?:budget of|under|below)\s*₹?(\d+)", query_lower)
    if budget_match:
        budget = int(budget_match.group(1))
        filtered_df = filtered_df[filtered_df["Price"] <= budget]

    # Domain filter
    for domain in df["Domain"].unique():
        if domain.lower() in query_lower:
            filtered_df = filtered_df[filtered_df["Domain"].str.lower() == domain.lower()]
            break

    # If no lawyers match after filtering
    if filtered_df.empty:
        print("No lawyers match your query after filtering.")
        return

    # Embed the query
    query_embedding = get_bert_embedding(query)

    # Compute similarity
    lawyer_embeddings = filtered_df["embedding"].tolist()
    sim_scores = cosine_similarity([query_embedding], lawyer_embeddings)[0]
    filtered_df["Similarity Score"] = sim_scores
    filtered_df = filtered_df.sort_values(by="Similarity Score", ascending=False)

    # Final display
    result = filtered_df[[
        "Name", "Domain", "Years of active experience", "Price",
        "Client satisfaction (out of 10)", "Similarity Score"
    ]].rename(columns={
        "Years of active experience": "Experience (yrs)",
        "Price": "Estimated Price (₹)",
        "Client satisfaction (out of 10)": "Satisfaction (/10)"
    })

    print(f"\n📋 Lawyers matching your query: \"{query}\"\n")
    print(result.to_string(index=False))

    return result

recommend("give me names of lawyers who have less than 20 years of experience in civil law for budget of 10000rs ")

import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
# Compute embeddings (limit to first 100 for speed)
embeddings = df["summary_text"].iloc[:100].apply(get_bert_embedding).tolist()

# Reduce dimensions for visualization
pca = PCA(n_components=2)
reduced_embeddings = pca.fit_transform(embeddings)

# Plot
plt.figure(figsize=(10, 7))
plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], c='blue', alpha=0.6)
for i, name in enumerate(df["Name"].iloc[:100]):
    plt.annotate(name.split()[0], (reduced_embeddings[i, 0], reduced_embeddings[i, 1]), fontsize=8, alpha=0.6)
plt.title("BERT Embeddings of Lawyer Summaries (PCA Projection)")
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.grid(True)
plt.tight_layout()
plt.show()

!pip install sentence-transformers

import re
import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer

# Load Sentence-BERT model
sbert_model = SentenceTransformer("all-MiniLM-L6-v2")  # You can try other SBERT models too
print("Sentence-BERT model loaded.")

# Load data
df = pd.read_csv("lawyers_with_estimated_price (1).csv")

# Create summary column
def create_summary(row):
    return (
        f"{row['Name']} is a lawyer specializing in {row['Domain']} with "
        f"{row['Years of active experience']} years of experience. "
        f"Charges approximately ₹{row['Price']:.0f} and has a client satisfaction rating of "
        f"{row['Client satisfaction (out of 10)']}/10."
    )

df["summary_text"] = df.apply(create_summary, axis=1)

# Precompute SBERT embeddings for all summaries
print("Generating SBERT embeddings for all lawyers...")
df["embedding"] = df["summary_text"].apply(lambda x: sbert_model.encode(x))

# Recommendation function
def recommend(query):
    filtered_df = df.copy()
    query_lower = query.lower()

    # Experience filter
    exp_match = re.search(r"(less|more) than (\d+)\s*years?", query_lower)
    if exp_match:
        op, val = exp_match.groups()
        val = int(val)
        if op == "less":
            filtered_df = filtered_df[filtered_df["Years of active experience"] < val]
        else:
            filtered_df = filtered_df[filtered_df["Years of active experience"] > val]

    # Budget filter
    budget_match = re.search(r"(?:budget of|under|below)\s*₹?(\d+)", query_lower)
    if budget_match:
        budget = int(budget_match.group(1))
        filtered_df = filtered_df[filtered_df["Price"] <= budget]

    # Domain filter
    for domain in df["Domain"].unique():
        if domain.lower() in query_lower:
            filtered_df = filtered_df[filtered_df["Domain"].str.lower() == domain.lower()]
            break

    if filtered_df.empty:
        print("No lawyers match your query after filtering.")
        return

    # Embed the query
    query_embedding = sbert_model.encode(query)

    # Compute similarity
    lawyer_embeddings = list(filtered_df["embedding"])
    sim_scores = cosine_similarity([query_embedding], lawyer_embeddings)[0]
    filtered_df["Similarity Score"] = sim_scores
    filtered_df = filtered_df.sort_values(by="Similarity Score", ascending=False)

    # Final display
    result = filtered_df[[
        "Name", "Domain", "Years of active experience", "Price",
        "Client satisfaction (out of 10)", "Similarity Score"
    ]].rename(columns={
        "Years of active experience": "Experience (yrs)",
        "Price": "Estimated Price (₹)",
        "Client satisfaction (out of 10)": "Satisfaction (/10)"
    })

    print(f"\n📋 Lawyers matching your query: \"{query}\"\n")
    print(result.to_string(index=False))

    return result

recommend("give me names of lawyers who have more than 20 years of experience in family law for budget of 25000rs ")

"""Graphs"""

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(8, 5))
sns.histplot(df['Years of active experience'], bins=10, kde=True)
plt.title("Distribution of Lawyers by Experience")
plt.xlabel("Years of Experience")
plt.ylabel("Number of Lawyers")
plt.grid(True)
plt.show()

plt.figure(figsize=(8, 5))
sns.boxplot(x=df["Price"])
plt.title("Lawyer Price Distribution")
plt.xlabel("Estimated Price (₹)")
plt.grid(True)
plt.show()

plt.figure(figsize=(10, 6))
sns.boxplot(x="Domain", y="Client satisfaction (out of 10)", data=df)
plt.xticks(rotation=45)
plt.title("Client Satisfaction Across Legal Domains")
plt.xlabel("Legal Domain")
plt.ylabel("Satisfaction (/10)")
plt.tight_layout()
plt.show()

plt.figure(figsize=(8, 6))
sns.scatterplot(x="Years of active experience", y="Price", hue="Domain", data=df)
plt.title("Experience vs. Estimated Price")
plt.xlabel("Experience (Years)")
plt.ylabel("Price (₹)")
plt.grid(True)
plt.show()

def plot_similarity_scores(result_df, query):
    plt.figure(figsize=(10, 5))
    sns.barplot(x="Name", y="Similarity Score", data=result_df)
    plt.title(f"Similarity Scores for Query: '{query}'")
    plt.ylabel("Cosine Similarity")
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

result_df = recommend("I want a divorce lawyer with more than 5 years experience and under ₹5000")
plot_similarity_scores(result_df, "I want a divorce lawyer with more than 5 years experience and under ₹5000")

from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns

# Convert embeddings list to array
import numpy as np
X = np.vstack(df["embedding"].values)

# Reduce dimensions to 2D using PCA
pca = PCA(n_components=2)
X_reduced = pca.fit_transform(X)

# Plot
plt.figure(figsize=(10, 6))
sns.scatterplot(x=X_reduced[:, 0], y=X_reduced[:, 1], hue=df["Domain"])
plt.title("2D Visualization of Lawyer Embeddings (PCA)")
plt.xlabel("PCA Component 1")
plt.ylabel("PCA Component 2")
plt.legend(title="Domain", bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()

def plot_similarity_scores(result_df, query):
    plt.figure(figsize=(10, 5))
    sns.barplot(x="Name", y="Similarity Score", data=result_df)
    plt.title(f"Similarity Scores for Query: '{query}'")
    plt.ylabel("Cosine Similarity")
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

# Example usage:
result_df = recommend("I want a criminal lawyer with more than 5 years of experience under ₹20000")
plot_similarity_scores(result_df, "I want a criminal lawyer with more than 5 years of experience under ₹20000")

import seaborn as sns
import numpy as np
import matplotlib.pyplot as plt

top_n = 10

# Ensure result_df is not None and has data
if result_df is not None and not result_df.empty:
    # Get the summaries for the top N lawyers from the original df
    # Filter the original df to get the rows present in result_df (based on Name for simplicity)
    # Note: A more robust way would be to merge or use index if the index is consistent
    top_lawyers_df = df[df['Name'].isin(result_df.head(top_n)['Name'])]

    # Re-calculate embeddings for these specific lawyers if the column is not in result_df
    # This avoids modifying the original recommend function output
    if 'embedding' not in top_lawyers_df.columns:
         # Assumes sbert_model is available from previous cells
         print("Warning: 'embedding' column not found in top_lawyers_df. Re-calculating embeddings.")
         top_lawyers_df["embedding"] = top_lawyers_df["summary_text"].apply(lambda x: sbert_model.encode(x))

    # Now, extract the embeddings
    top_embeddings = np.vstack(top_lawyers_df.head(top_n)["embedding"].values)

    plt.figure(figsize=(12, 6))
    sns.heatmap(top_embeddings, cmap="coolwarm", cbar=True)
    plt.title(f"Heatmap of SBERT Embeddings for Top {top_n} Lawyers")
    plt.xlabel("Embedding Dimensions")
    plt.ylabel("Lawyer Rank")
    plt.show()
else:
    print("result_df is empty or None. Cannot generate heatmap.")

def plot_accuracy_at_k(evaluation_data, max_k=10):
    accuracies = []
    ks = list(range(1, max_k + 1))
    for k in ks:
        acc = evaluate_accuracy_at_k(evaluation_data, k)
        print(f"Accuracy@{k}: {acc:.2f}")
        accuracies.append(acc)

    plt.figure(figsize=(8, 5))
    plt.plot(ks, accuracies, marker='o', linestyle='--', color='royalblue', label='Accuracy@K')
    plt.fill_between(ks, accuracies, alpha=0.1, color='blue')
    plt.title("Recommendation Accuracy vs Top-K")
    plt.xlabel("Top-K (K)")
    plt.ylabel("Accuracy")
    plt.ylim(0, 1.05)
    plt.xticks(ks)
    plt.grid(True)
    plt.legend()
    plt.tight_layout()
    plt.show()

evaluation_data = [
    {"query": "divorce under 10000", "expected_lawyer": "Amit Sharma"},
    {"query": "criminal experience more than 10 years", "expected_lawyer": "Priya Mehra"},
    {"query": "property lawyer under 30000", "expected_lawyer": "Ravi Kapoor"},
    # Add more realistic cases here...
]

plot_accuracy_at_k(evaluation_data, max_k=5)

import time

query = "lawyer  under ₹50000 in criminal law"

start_time = time.time()
result = recommend(query)
end_time = time.time()

print(f"\nExecution time: {end_time - start_time:.4f} seconds")

import time
import pandas as pd
import numpy as np

sizes = [100, 500, 1000, 2000]  # Simulated dataset sizes
query = "criminal lawyer under ₹50000 with more than 5 years experience"

times = []

for size in sizes:
    # Simulate larger data by duplicating rows
    test_df = pd.concat([df]*((size // len(df)) + 1), ignore_index=True).head(size)

    # Recompute embeddings if needed
    test_df["embedding"] = test_df["summary_text"].apply(lambda x: sbert_model.encode(x))

    start = time.time()

    # Subset the dataframe for recommend() to use
    def recommend_test(query, df_input):
        filtered_df = df_input.copy()
        # Filtering and query processing logic here...
        # (Copy recommend() internals and just use df_input instead of df)
        return

    recommend_test(query, test_df)

    end = time.time()
    times.append(end - start)

# Plot or print results
for s, t in zip(sizes, times):
    print(f"Size: {s}, Time: {t:.4f} sec")

!pip install memory_profiler
from memory_profiler import memory_usage

def run_recommend():
    # Make sure the 'recommend' function is accessible in the scope
    # It's defined in a previous cell, so it should be available
    recommend("some test query")

mem_usage = memory_usage(run_recommend)
print(f"Memory used: {max(mem_usage) - min(mem_usage):.2f} MB")

import matplotlib.pyplot as plt

plt.plot(sizes, times, marker='o')
plt.xlabel("Dataset Size (lawyers)")
plt.ylabel("Execution Time (seconds)")
plt.title("Scalability of Recommendation Function")
plt.grid(True)
plt.show()

def average_similarity_at_k(result_df, k=20):
    return result_df.head(k)["Similarity Score"].mean()

test_queries = [
    "criminal lawyer under ₹50000 with more than 5 years experience",

]

similarities = []

for query in test_queries:
    result = recommend(query)
    if result is not None:
        avg_sim = average_similarity_at_k(result, k=20)
        print(f"Query: {query}\nAverage Similarity@5: {avg_sim:.4f}\n")
        similarities.append(avg_sim)

if similarities:
    overall_avg = sum(similarities) / len(similarities)
    print(f"🔍 Overall Average Similarity@5: {overall_avg:.4f}")

